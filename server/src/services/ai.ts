/**
 * AI æœåŠ¡
 * ä½¿ç”¨ OpenAI SDK è°ƒç”¨ DeepSeek API
 */

import OpenAI from 'openai'
import { getAIConfig } from '../config/ai.js'

export interface ChatMessage {
  role: 'system' | 'user' | 'assistant'
  content: string
}

export interface ChatOptions {
  messages: ChatMessage[]
  model?: string  // æ¨¡å‹é€‰æ‹©
  temperature?: number
  maxTokens?: number
  stream?: boolean
}

/**
 * åˆ›å»º OpenAI å®¢æˆ·ç«¯
 * æ ¹æ®æ¨¡å‹è‡ªåŠ¨é€‰æ‹© API ç«¯ç‚¹
 */
function createOpenAIClient(model: string) {
  const config = getAIConfig()
  
  // æ ¹æ®æ¨¡å‹é€‰æ‹© API é…ç½®
  let apiKey = config.apiKey
  let baseURL = config.baseURL
  
  // Kimi (Moonshot) æ¨¡å‹
  if (model.startsWith('moonshot-')) {
    apiKey = process.env.MOONSHOT_API_KEY || config.apiKey
    baseURL = 'https://api.moonshot.cn/v1'
    console.log('ğŸŒ™ ä½¿ç”¨ Kimi API')
  }
  // DeepSeek æ¨¡å‹
  else if (model.startsWith('deepseek-')) {
    apiKey = process.env.DEEPSEEK_API_KEY || config.apiKey
    baseURL = 'https://api.deepseek.com'
    console.log('ğŸ¤– ä½¿ç”¨ DeepSeek API')
  }
  
  return new OpenAI({
    apiKey,
    baseURL,
    timeout: 120000,
    maxRetries: 3,
    fetch: (url, init) => {
      return fetch(url, {
        ...init,
        keepalive: true,
      })
    },
  })
}

/**
 * å‘é€èŠå¤©è¯·æ±‚ï¼ˆæµå¼ï¼‰
 */
export async function* streamChat(options: ChatOptions) {
  const { messages, model, temperature = 0.7, maxTokens = 2000 } = options
  const config = getAIConfig()
  
  // ä½¿ç”¨ä¼ å…¥çš„æ¨¡å‹ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é…ç½®çš„æ¨¡å‹
  const selectedModel = model || config.model
  const openai = createOpenAIClient(selectedModel)

  try {
    console.log('ğŸ¤– å¼€å§‹ AI è¯·æ±‚:', {
      model: selectedModel,
      messageCount: messages.length,
      temperature,
      maxTokens,
    })

    const stream = await openai.chat.completions.create({
      model: selectedModel,
      messages,
      temperature,
      max_tokens: maxTokens,
      stream: true,
    })

    let chunkCount = 0
    let logFile = ''
    
    for await (const chunk of stream) {
      const delta = chunk.choices[0]?.delta as any
      
      if (chunkCount < 3) {
        const chunkLog = JSON.stringify(chunk, null, 2)
        logFile += `\n=== Chunk ${chunkCount + 1} ===\n${chunkLog}\n`
        console.log(`ğŸ“¦ Chunk ${chunkCount + 1}:`, chunkLog)
      }
      
      // å¤„ç†æ€è€ƒè¿‡ç¨‹ï¼ˆreasoning_contentï¼‰- DeepSeek ç‰¹æœ‰
      if (delta?.reasoning_content) {
        chunkCount++
        console.log('ğŸ’­ [æ€è€ƒ]:', delta.reasoning_content.substring(0, 50))
        yield JSON.stringify({
          type: 'reasoning',
          content: delta.reasoning_content,
        })
      }
      
      // å¤„ç†æ­£å¸¸å†…å®¹
      if (delta?.content) {
        chunkCount++
        yield JSON.stringify({
          type: 'content',
          content: delta.content,
        })
      }
    }
    
    if (logFile) {
      console.log('\n' + '='.repeat(50))
      console.log('å‰3ä¸ª chunk çš„å®Œæ•´ç»“æ„å·²è®°å½•åœ¨ä¸Šæ–¹')
      console.log('='.repeat(50) + '\n')
    }

    console.log('âœ… AI è¯·æ±‚å®Œæˆï¼Œå…±ç”Ÿæˆ', chunkCount, 'ä¸ª chunk')
  } catch (error: any) {
    console.error('âŒ AI æœåŠ¡é”™è¯¯:', {
      message: error.message,
      code: error.code,
      status: error.status,
      type: error.type,
    })

    if (error.code === 'ECONNRESET' || error.code === 'ETIMEDOUT') {
      throw new Error('ç½‘ç»œè¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ–ç¨åé‡è¯•')
    } else if (error.status === 401) {
      throw new Error('API Key æ— æ•ˆï¼Œè¯·æ£€æŸ¥é…ç½®')
    } else if (error.status === 429) {
      throw new Error('è¯·æ±‚è¿‡äºé¢‘ç¹ï¼Œè¯·ç¨åé‡è¯•')
    } else if (error.status === 500) {
      throw new Error('AI æœåŠ¡å™¨é”™è¯¯ï¼Œè¯·ç¨åé‡è¯•')
    } else {
      throw new Error(error.message || 'AI æœåŠ¡é”™è¯¯')
    }
  }
}

/**
 * å‘é€èŠå¤©è¯·æ±‚ï¼ˆéæµå¼ï¼‰
 */
export async function chat(options: ChatOptions): Promise<string> {
  const { messages, model, temperature = 0.7, maxTokens = 2000 } = options
  const config = getAIConfig()
  const selectedModel = model || config.model
  const openai = createOpenAIClient(selectedModel)

  try {
    const response = await openai.chat.completions.create({
      model: selectedModel,
      messages,
      temperature,
      max_tokens: maxTokens,
      stream: false,
    })

    return response.choices[0]?.message?.content || ''
  } catch (error: any) {
    console.error('âŒ AI æœåŠ¡é”™è¯¯:', error)
    throw error
  }
}
